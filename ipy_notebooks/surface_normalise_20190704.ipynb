{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gatesk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pysal\\model\\spvcm\\abstracts.py:10: UserWarning: The `dill` module is required to use the sqlite backend fully.\n",
      "  from .sqlite import head_to_sql, start_sql\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [8, 8]\n",
    "\n",
    "from faker import Factory\n",
    "from itertools import islice\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import pyodbc\n",
    "import pysal as ps\n",
    "\n",
    "from pysal.viz.mapclassify import Natural_Breaks as nb\n",
    "\n",
    "from math import log10\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "fake = Factory.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMITED READ\n",
    "\n",
    "conn = pyodbc.connect('DSN=GEODWH;UID=GEODWH_READ;PWD=readonly')\n",
    "# ss_select = \"SELECT [RPT_ID],[RIN],[RPT_TYPE],[SAMPLEID],[SAMPCODE],[FILE_ID],[LAT94],[LNG94],[As_ppm],[Au_ppm] FROM [GEODWH].[dbo].[ASSAY_SURFSAMP_TRACE] WHERE [LNG94] > 140 and [LNG94] < 142 and [LAT94] > -32.5 and [LAT94] < -31 and SAMPCODE in ('SOIL', 'AUGER', 'SURF_DRILL', 'LAG', 'FLOAT', 'UNKNOWN','VEGETATION')\"\n",
    "# dh_select = \"SELECT [RPT_ID],[RIN],[RPT_TYPE],[HOLEID] AS SAMPLEID,[DRILLCODE] AS SAMPCODE,[ASSAY_FILE_ID] AS FILEID,[LAT94],[LNG94],[As_ppm],[Au_ppm] FROM [GEODWH].[dbo].[ASSAY_MAX_DRILL_TRACE] WHERE DRILLCODE in ('AUGER', 'VAC', 'SURF_DRILL', 'VIB', 'AUG') and [LNG94] > 140 and [LNG94] < 142 and [LAT94] > -32.5 and [LAT94] < -31\"\n",
    "\n",
    "# # Read the Auger samples from the drillhole table\n",
    "# # We only use : 'SOIL', 'AUGER', 'SURF_DRILL', 'LAG', 'FLOAT', 'UNKNOWN','VEGETATION'\n",
    "\n",
    "# drillholes = pd.read_sql(dh_select, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = pyodbc.connect('DSN=GEODWH;UID=GEODWH_READ;PWD=readonly')\n",
    "\n",
    "# ss_select = \"SELECT [RPT_ID],[RIN],[RPT_TYPE],[SAMPLEID],[SAMPCODE],[FILE_ID],[LAT94],[LNG94],[As_ppm],[Au_ppm] FROM [GEODWH].[dbo].[ASSAY_SURFSAMP_TRACE] WHERE SAMPCODE in ('SOIL', 'AUGER', 'SURF_DRILL', 'LAG', 'FLOAT', 'UNKNOWN','VEGETATION')\"\n",
    "# dh_select = \"SELECT [RPT_ID],[RIN],[RPT_TYPE],[HOLEID] AS SAMPLEID,[DRILLCODE] AS SAMPCODE,[ASSAY_FILE_ID] AS FILEID,[LAT94],[LNG94],[As_ppm],[Au_ppm] FROM [GEODWH].[dbo].[ASSAY_MAX_DRILL_TRACE] WHERE DRILLCODE in ('AUGER', 'VAC', 'SURF_DRILL', 'VIB', 'AUG')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read from the database\n",
    "# dh = pd.read_sql(dh_select, conn)\n",
    "# ss = pd.read_sql(ss_select, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence = \"SELECT [MR_AUTHOR], [MR_CONFIDENTIALITY],[MR_RECTYPE],[MR_REPYEAR],[MR_RIN] FROM [GEODWH].[dbo].[DG_MINREPS] where MR_RECTYPE = 'MINERAL'\"\n",
    "\n",
    "# rpt_confidence = pd.read_sql(confidence, conn)\n",
    "# rpt_confidence.head()\n",
    "\n",
    "# # Pickle them up for next time\n",
    "# dsfolder = os.path.join(os.path.dirname(os.getcwd()), 'data_src')\n",
    "\n",
    "# with open(os.path.join(dsfolder, 'rpt_confidence.pickle'), 'wb') as f:\n",
    "#     pickle.dump(rpt_confidence, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickle them up for next time\n",
    "# dsfolder = os.path.join(os.path.dirname(os.getcwd()), 'data_src')\n",
    "\n",
    "# with open(os.path.join(dsfolder, 'dh.pickle'), 'wb') as f:\n",
    "#     pickle.dump(dh, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open(os.path.join(dsfolder, 'ss.pickle'), 'wb') as f:\n",
    "#     pickle.dump(ss, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfolder = os.path.join(os.path.dirname(os.getcwd()), 'data_src')\n",
    "\n",
    "# Read the pickles\n",
    "with open(os.path.join(dsfolder, 'drillholes.pickle'), 'rb') as fin:\n",
    "    dh = pickle.load(fin)\n",
    "    \n",
    "with open(os.path.join(dsfolder, 'ss.pickle'), 'rb') as fin:\n",
    "    ss = pickle.load(fin)\n",
    "    \n",
    "with open(os.path.join(dsfolder, 'rpt_confidence.pickle'), 'rb') as fin:\n",
    "    rpt_confidence = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique sample codes\n",
    "dhtypes = dh['SAMPCODE'].unique()\n",
    "dh['SAMPCODE'].replace('AUG', 'AUGER', inplace=True) # Fix a namecase\n",
    "dh['SAMPCODE'].replace('Auger', 'AUGER', inplace=True) # Fix a namecase\n",
    "dh['SAMPCODE'].replace('auger', 'AUGER', inplace=True) # Fix a namecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AUGER' 'SURF_DRILL' 'VIB' 'VAC']\n"
     ]
    }
   ],
   "source": [
    "dhtypes = dh['SAMPCODE'].unique()\n",
    "print(dhtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss['SAMPCODE'] = ss['SAMPCODE'].map(lambda x: x.strip())# Fix some whitespaces\n",
    "ss['RIN'] = ss['RIN'].map(lambda x: x.strip())\n",
    "ss['RPT_TYPE'] = ss['RPT_TYPE'].map(lambda x: x.strip())\n",
    "ss['SAMPLEID'] = ss['SAMPLEID'].map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss['SAMPCODE'].replace('Soil', 'SOIL', inplace=True)\n",
    "ss['SAMPCODE'].replace('soil', 'SOIL', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOIL' 'SURF_DRILL' 'FLOAT' 'UNKNOWN' 'LAG' 'AUGER' 'VEGETATION']\n"
     ]
    }
   ],
   "source": [
    "sstypes = ss['SAMPCODE'].unique()\n",
    "print(sstypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gatesk\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample types: ['AUGER' 'FLOAT' 'LAG' 'SOIL' 'SURF_DRILL' 'UNKNOWN' 'VAC' 'VEGETATION'\n",
      " 'VIB']\n",
      "700554\n"
     ]
    }
   ],
   "source": [
    "# Combine the drilling with the surface samples\n",
    "samples = ss.append(dh)\n",
    "\n",
    "# Check unique sample codes\n",
    "stypes = samples['SAMPCODE'].unique()\n",
    "stypes.sort()\n",
    "\n",
    "print(f'Sample types: {stypes}')\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfolder = os.path.join(os.path.dirname(os.getcwd()), 'data_src')\n",
    "\n",
    "with open(os.path.join(dsfolder, 'samples.pickle'), 'wb') as f:\n",
    "    pickle.dump(samples, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dsfolder, 'samples.pickle'), 'rb') as f:\n",
    "    samps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700554\n"
     ]
    }
   ],
   "source": [
    "# Take a look\n",
    "\n",
    "print(len(samps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>As_ppm</th>\n",
       "      <th>Au_ppm</th>\n",
       "      <th>FILEID</th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>LAT94</th>\n",
       "      <th>LNG94</th>\n",
       "      <th>RIN</th>\n",
       "      <th>RPT_ID</th>\n",
       "      <th>RPT_TYPE</th>\n",
       "      <th>SAMPCODE</th>\n",
       "      <th>SAMPLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.4</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>286.0</td>\n",
       "      <td>-31.860818</td>\n",
       "      <td>141.665005</td>\n",
       "      <td>R00029271</td>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>SOIL</td>\n",
       "      <td>370056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>286.0</td>\n",
       "      <td>-31.860389</td>\n",
       "      <td>141.665091</td>\n",
       "      <td>R00029271</td>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>SOIL</td>\n",
       "      <td>370057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>0.023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>286.0</td>\n",
       "      <td>-31.862621</td>\n",
       "      <td>141.663997</td>\n",
       "      <td>R00029271</td>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>SOIL</td>\n",
       "      <td>370058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>286.0</td>\n",
       "      <td>-31.862969</td>\n",
       "      <td>141.663766</td>\n",
       "      <td>R00029271</td>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>SOIL</td>\n",
       "      <td>370059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>286.0</td>\n",
       "      <td>-31.857557</td>\n",
       "      <td>141.665381</td>\n",
       "      <td>R00029271</td>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>SOIL</td>\n",
       "      <td>370060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   As_ppm  Au_ppm  FILEID  FILE_ID      LAT94       LNG94        RIN  RPT_ID  \\\n",
       "0     2.4   0.002     NaN    286.0 -31.860818  141.665005  R00029271      28   \n",
       "1     2.1   0.003     NaN    286.0 -31.860389  141.665091  R00029271      28   \n",
       "2     3.3   0.023     NaN    286.0 -31.862621  141.663997  R00029271      28   \n",
       "3     3.0   0.021     NaN    286.0 -31.862969  141.663766  R00029271      28   \n",
       "4     3.2  -0.001     NaN    286.0 -31.857557  141.665381  R00029271      28   \n",
       "\n",
       "  RPT_TYPE SAMPCODE SAMPLEID  \n",
       "0        A     SOIL   370056  \n",
       "1        A     SOIL   370057  \n",
       "2        A     SOIL   370058  \n",
       "3        A     SOIL   370059  \n",
       "4        A     SOIL   370060  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_full = pd.merge(samps, rpt_confidence, how='inner', left_on='RIN', right_on='MR_RIN', left_index=False, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_full.drop(['MR_RIN'], inplace=True, axis=1)\n",
    "samples_full.drop(['MR_RECTYPE'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj\n",
    "from pyproj import transform\n",
    "\n",
    "geographic = Proj(init='epsg:4283')\n",
    "projected = Proj(init='epsg:3112')\n",
    "\n",
    "def lat(coords):\n",
    "    E, N = transform(geographic, projected, coords[1], coords[0])\n",
    "    return E\n",
    "\n",
    "def lon(coords):\n",
    "    E, N = transform(geographic, projected, coords[1], coords[0])\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_full['EASTING'] = samples_full.apply(lambda x: lat((x['LAT94'], x['LNG94'])), axis=1)\n",
    "samples_full['NORTHING'] = samples_full.apply(lambda x: lon((x['LAT94'], x['LNG94'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make geometry field wkt (Well Known Text)\n",
    "samples_full['Coordinates'] = list(zip(samples_full.EASTING, samples_full.NORTHING)) # Round to 6dp\n",
    "samples_full['Coordinates'] = samples_full['Coordinates'].apply(Point)\n",
    "samples_full = gpd.GeoDataFrame(samples_full, geometry='Coordinates', crs={'init' :'epsg:3112'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_full.head()\n",
    "\n",
    "with open(os.path.join(dsfolder, 'samples_conditioned.pickle'), 'wb') as f:\n",
    "    pickle.dump(samples_full, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dsfolder, 'samples_conditioned.pickle'), 'rb') as f:\n",
    "    samps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Buffer the points\n",
    "\n",
    "def buffer(data, distance=0.0075, resolution=10):\n",
    "    \"\"\"\n",
    "    Buffer points to return polygons of spatially distinct data sets.\n",
    "    \n",
    "    params:\n",
    "        data: GeoDataFrame\n",
    "    returns:\n",
    "        buffer_union: GeoDataFrame of buffers\n",
    "    \"\"\"\n",
    "    \n",
    "    buffers = data.buffer(distance, resolution)\n",
    "    buffer_union = buffers.unary_union\n",
    "    return buffer_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the single buffer polygons to a GeoDataFrame\n",
    "\n",
    "def sb_attributes(singlebuffer):\n",
    "    \"\"\"\n",
    "    Converts wkt polygon to GeoDataFrame\n",
    "    \n",
    "    params:\n",
    "        wkt: Polygon\n",
    "    returns:\n",
    "        GeoDataFrame\n",
    "    \"\"\"\n",
    "    colour = fake.hex_color()\n",
    "    \n",
    "    # Set a dictonary to build the attributes for the polygon\n",
    "    sb_attributes = dict()\n",
    "    sb_attributes['geometry'] = singlebuffer\n",
    "    sb_attributes['index'] = [0]\n",
    "    sb_attributes['stroke'] = [colour]\n",
    "    # TODO add the RIN as a tooltip\n",
    "    \n",
    "    # Covert the dictionary into the GeoDataFrame \n",
    "    sb_attributes_df = pd.DataFrame(sb_attributes)\n",
    "    sb_attributes_gdf = gpd.GeoDataFrame(sb_attributes_df)\n",
    "    sb_attributes_gdf.columns={'geometry': 'geometry', 'index': 'index', 'stroke': 'stroke'}\n",
    "    sb_attributes_gdf.crs = {'init' :'epsg:4326'}\n",
    "    \n",
    "    return sb_attributes_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the multi buffers polygons to a GeoDataFrame\n",
    "\n",
    "def mb_attributes(multibuffer):\n",
    "        \"\"\"\n",
    "        Converts wkt MultiPolygon to GeoDataFrame\n",
    "\n",
    "        params:\n",
    "            wkt: MultiPolygon\n",
    "        returns:\n",
    "            GeoDataFrame\n",
    "        \"\"\"\n",
    "        # Set a dictonary to build the attributes for each polygon\n",
    "        mb_attributes = dict()\n",
    "        geometry = []\n",
    "        index = []\n",
    "        stroke = []\n",
    "    \n",
    "        # Loop over each polygon in the MultiPolygon\n",
    "        for idx, b in enumerate(multibuffer):\n",
    "            colour = fake.hex_color()\n",
    "            stroke.append(colour)\n",
    "            geometry.append(b)\n",
    "            index.append(idx)\n",
    "            # TODO add the RIN as a tooltip\n",
    "\n",
    "            # Populate the dictionary\n",
    "            mb_attributes['geometry'] = geometry\n",
    "            mb_attributes['index'] = index\n",
    "            mb_attributes['stroke'] = stroke\n",
    "            \n",
    "        # Covert the dictionary into the GeoDataFrame \n",
    "        mb_attributes_df = pd.DataFrame(mb_attributes)\n",
    "        mb_attributes_gdf = gpd.GeoDataFrame(mb_attributes_df)\n",
    "        mb_attributes_gdf.columns={'geometry': 'geometry', 'index': 'index', 'stroke': 'stroke'}\n",
    "        mb_attributes_gdf.crs = {'init' :'epsg:4326'}\n",
    "        \n",
    "        return mb_attributes_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to join the points to the buffers\n",
    "\n",
    "def rin_stype_buffered(points, buffers):\n",
    "    \"\"\"\n",
    "    Spatially join points to buffers\n",
    "\n",
    "    params:\n",
    "        points: GeoDataFrame containing Points\n",
    "        buffers: GeoDataFrame containing Points\n",
    "    returns:\n",
    "        Combined GeoDataFrame of points and buffers\n",
    "    \"\"\"\n",
    "    points_buffered = gpd.sjoin(points, buffers, how=\"inner\", op='intersects')\n",
    "    points_buffered.drop(['index_right'], axis=1)\n",
    "    return points_buffered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peep at the head of the DataFrame\n",
    "print(f'\\nSample count : {len(points)}\\n')\n",
    "print(samples.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peep at the gold stats\n",
    "print(samples['Au_ppm'].describe())\n",
    "samples['Au_ppm'].hist(range = (0,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function to subdivide the samples according to rin and sampletype\n",
    "\n",
    "def data_divisions(rins):\n",
    "    \"\"\"Divide the data\n",
    "    \n",
    "    params:\n",
    "        Groupby object of samples grouped by rin\n",
    "    returns:\n",
    "        yields a tuple (rin, sampleType, sampleData)\"\"\"\n",
    "    # For each rin...\n",
    "    for rin, rindata in rins:\n",
    "        stypes = rindata.groupby(['SAMPCODE'])\n",
    "        \n",
    "        # For each sample type...\n",
    "        for stype, rin_stype_data in stypes:\n",
    "            yield (rin, stype, rin_stype_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the rins\n",
    "samples.sort_values('RIN')\n",
    "\n",
    "# Groupby the rin\n",
    "rins = samples.groupby(['RIN'])\n",
    "\n",
    "# Empty frames to collect the buffers and processed samples\n",
    "all_buffers = gpd.GeoDataFrame()\n",
    "sbuffered_samples = gpd.GeoDataFrame()\n",
    "mbuffered_samples = gpd.GeoDataFrame()\n",
    "all_buffered_samples = gpd.GeoDataFrame()\n",
    "\n",
    "# Counter to check the output is valid\n",
    "total_points = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the samples grouped by the rins and yield the subdivisions\n",
    "for rin, stype, subsamples in data_divisions(rins):\n",
    "    \n",
    "    # We buffer all the samples\n",
    "    buffers = buffer(subsamples)\n",
    "    \n",
    "    print(rin, stype, len(subsamples), buffers.geom_type)\n",
    "    total_samples = total_samples + len(subsamples)\n",
    "    if buffers.geom_type == 'Polygon':\n",
    "        \n",
    "        # For each of the single wkt polygons convert to a dataframe (adding attributes)\n",
    "        _df = sb_attributes(buffers)\n",
    "        assert isinstance(df, gpd.GeoDataFrame)\n",
    "\n",
    "        # Aggregate the buffers\n",
    "        all_buffers = gpd.GeoDataFrame(pd.concat([all_buffers, _df], ignore_index=True))\n",
    "        \n",
    "        # Spatial join the samples to the buffer\n",
    "        sbuffered_samples = gpd.sjoin(_df, subsamples, how=\"inner\", op='intersects')\n",
    "        \n",
    "        # Aggregate the samples\n",
    "        all_buffered_samples = gpd.GeoDataFrame(pd.concat([all_buffered_samples, sbuffered_samples], ignore_index=True))\n",
    "\n",
    "    if buffers.geom_type == 'MultiPolygon':\n",
    "        \n",
    "        # Split the Multipolygon wkt to dataframe\n",
    "        _mdf = mb_attributes(buffers)\n",
    "\n",
    "        # Set empty frames to collect the subdivided sets\n",
    "        multisamples = gpd.GeoDataFrame()\n",
    "        mbuffers = gpd.GeoDataFrame()\n",
    "        \n",
    "        # Iterate over the multi-buffer dataframe\n",
    "        for row, mbuffer in _mdf.groupby('index'):\n",
    "            assert isinstance(mbuffer, gpd.GeoDataFrame)\n",
    "\n",
    "            # Aggregate the buffers\n",
    "            mbuffers = gpd.GeoDataFrame(pd.concat([mbuffers, mbuffer], ignore_index=True))\n",
    "            \n",
    "            # Spatially join the samples to the buffers\n",
    "            mbuffered_samples = gpd.sjoin(mbuffer, subsamples, how=\"inner\", op='intersects')\n",
    "            \n",
    "            # Aggregate the samples\n",
    "            multisamples = gpd.GeoDataFrame(pd.concat([multisamples, mbuffered_samples], ignore_index=True))\n",
    "        \n",
    "        # Aggregate the single and multipolygon buffers and samples\n",
    "        all_buffers = gpd.GeoDataFrame(pd.concat([all_buffers, mbuffers], ignore_index=True))\n",
    "        all_buffered_samples = gpd.GeoDataFrame(pd.concat([all_buffered_samples, multisamples], ignore_index=True))\n",
    "        \n",
    "        # Watch it grow\n",
    "        print(f'\\nCaptured samples: {total_samples} of {len(samples)}\\n')\n",
    "\n",
    "# Assert it worked\n",
    "assert len(all_buffered_samples) == total_samples == samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buffers.to_file('ALL_BUFFERS.geojson', driver = \"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fix_data(object):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, df, element):\n",
    "        self.df = df\n",
    "        self.element = element\n",
    "    \n",
    "    def force_numeric(self):\n",
    "        # Coerce element to numeric values\n",
    "        self.df[self.element].fillna(0, inplace=True)\n",
    "        # Coerce element to numeric values\n",
    "        self.df[f'{self.element}_nn'] = pd.to_numeric(self.df[self.element])\n",
    "\n",
    "    def neg_conversions(self, x):\n",
    "        # Define the negative conversions\n",
    "        if x < -5:\n",
    "            return 0.0005\n",
    "        if x < 0:\n",
    "            return abs(x)/2\n",
    "        if x == 0:\n",
    "            return 0.0005\n",
    "        if x > 0:\n",
    "            return x\n",
    "        \n",
    "    def fix_neg(self):\n",
    "        self.df[f'{self.element}_fn'] = self.df[self.element].apply(lambda x: self.neg_conversions(x))\n",
    "\n",
    "    def log(self):\n",
    "        self.df[f'{self.element}_log'] = self.df[f'{self.element}_fn'].apply(lambda x: log10(x))\n",
    "        \n",
    "    def threshold(self):\n",
    "        median = self.df[f'{self.element}_log'].median()\n",
    "        # Mean Absolute Deviation: Tukey, J.W., 1977. Exploratory Data Analysis. Addison-Wesley, Reading, 688 pp\n",
    "        mad = self.df[f'{self.element}_log'].mad()  \n",
    "        # Set threshold: http://crcleme.org.au/Pubs/guides/gawler/a7_id_anomalies.pdf\n",
    "        self.threshold = median + 2*mad \n",
    "        \n",
    "    def scaling(self, x):\n",
    "        # Normalisation of values between 0 and 1\n",
    "        min_value = self.df[f'{self.element}_log'].min()\n",
    "        x = (x - min_value) / (self.threshold - min_value) # 'Max' value is the threshold \n",
    "        return x\n",
    "        \n",
    "    def apply_scaling(self):\n",
    "        self.df['normalised'] = self.df[f'{self.element}_log'].apply(lambda x: self.scaling(x))\n",
    "\n",
    "    def natural_breaks(self):\n",
    "        # Apply hex colours to natural breaks\n",
    "        classifier = nb(self.df[self.element], 7) # nb are natural breaks\n",
    "        self.df['classifications'] = self.df[self.element].apply(classifier)\n",
    "        self.df.classifications.replace([1,2,3,4,5,6,7], ['#82817d','#55b1d9','#5bd955','#e6a94e','#e02d2d','#da2de0','#af00b5'], inplace=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots(stats_dict, rin, samples):\n",
    "    vals = samples[f'{element}'].values\n",
    "    boxplot = mpl.pyplot.boxplot(vals)\n",
    "    stats_dict[rin] = boxplot\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised = gpd.GeoDataFrame()\n",
    "\n",
    "for subset, points in all_buffered_points.groupby(['RIN','index']):\n",
    "    fd = fix_data(points, 'Au_ppm')\n",
    "    fd.force_numeric()\n",
    "    fd.fix_neg()\n",
    "    fd.log()\n",
    "    fd.threshold()\n",
    "    fd.apply_scaling()\n",
    "    fd.natural_breaks()\n",
    "    print(len(normalised))\n",
    "    normalised = gpd.GeoDataFrame(pd.concat([normalised, points], ignore_index=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gdf = gpd.GeoDataFrame(normalised, crs={'init' :'epsg:4283'})\n",
    "\n",
    "norm = _gdf[['RIN', 'SAMPCODE', 'index', 'LAT94', 'LNG94', 'Au_ppm', 'normalised', 'classifications', 'stroke']]\n",
    "\n",
    "norm['normalised'].fillna(0, inplace=True)\n",
    "norm['classifications'].replace([0], '#82817d', inplace=True)\n",
    "\n",
    "norm['Coords'] = list(zip(norm.LNG94.round(6), norm.LAT94.round(6)))\n",
    "norm['Coords'] = norm['Coords'].apply(Point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(zip(norm.LAT94.values, norm.LNG94.values, norm.normalised.values, norm.classifications.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show me the buffers !!!!\n",
    "m = folium.Map(location=[-32, 141.5], zoom_start=9)\n",
    "\n",
    "samples = folium.FeatureGroup(name=\"samples\")\n",
    "\n",
    "sample_points = list(zip(norm.LAT94.values, norm.LNG94.values, norm.normalised.values, norm.classifications.values))\n",
    "\n",
    "for lat, lng, au, colour in sample_points:\n",
    "    samples.add_child(folium.CircleMarker(location=[lat, lng ], radius=1,\n",
    "        popup=str(au), \n",
    "        tooltip=str(au),\n",
    "        fill=True,  # Set fill to True\n",
    "        color=str(colour),\n",
    "        fill_opacity=0.5)).add_to(m)\n",
    "\n",
    "folium.GeoJson('ALL_BUFFERS.geojson',\n",
    "    style_function=lambda x: {\n",
    "        'color' : 'grey',\n",
    "        'weight' : 2,\n",
    "        'opacity': 0.66,\n",
    "        'fillColor' : x['properties']['stroke'],\n",
    "        }).add_to(m)\n",
    "\n",
    "# m.add_child(samples)\n",
    "m.add_child(folium.LayerControl())\n",
    "m.save(\"ALL_SAMPLES.html\")\n",
    "\n",
    "# IFrame(src='ALL_SAMPLES.html', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = all_buffered_points.groupby(['RIN','index'])\n",
    "\n",
    "def apply_normalisation(subset):\n",
    "    fd = fix_data(subset, 'Au_ppm')\n",
    "    fd.force_numeric()\n",
    "    fd.fix_neg()\n",
    "    fd.log()\n",
    "    fd.threshold()\n",
    "    fd.apply_scaling()\n",
    "    fd.natural_breaks()\n",
    "\n",
    "normalised = subsets.apply(apply_normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buffers.to_file('all_buffers.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(all_buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the points to the buffers\n",
    "\n",
    "buffered_points = gpd.sjoin(all_buffers, points, how=\"inner\", op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
